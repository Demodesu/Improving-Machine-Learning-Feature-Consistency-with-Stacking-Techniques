{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import sklearn\n",
    "import shap \n",
    "import time\n",
    "import math\n",
    "import seaborn as sns\n",
    "import pathlib\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_size_plot = 22\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "plt.rcParams['figure.labelweight'] = 'bold'\n",
    "plt.rcParams['figure.titleweight'] = 'bold'\n",
    "plt.rcParams['font.size'] = font_size_plot\n",
    "plt.rcParams['axes.formatter.useoffset'] = False\n",
    "\n",
    "path = os.path.abspath('')\n",
    "os.chdir(f'{path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_name_list = ['Handcode','Ascending','Descending']\n",
    "meta_model_list = ['DT','KNN','LIN']\n",
    "feature_importance_analysis_list = ['FPI','PDP','SHAP']\n",
    "metrics_list = ['MAE','MSE','RMSE','R2','MAPE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plot_normal_prediction_dataframe = pd.DataFrame()\n",
    "all_plot_fpi_prediction_dataframe = pd.DataFrame()\n",
    "all_plot_pdp_prediction_dataframe = pd.DataFrame()\n",
    "all_plot_shap_prediction_dataframe = pd.DataFrame()\n",
    "\n",
    "for method_name in method_name_list:\n",
    "    method_path = '\\\\'.join(path.split('\\\\')[:-1])+'\\\\'+'tuned model'+'\\\\'+method_name\n",
    "    for meta_model in meta_model_list:\n",
    "        for feature_importance_analysis in feature_importance_analysis_list:\n",
    "            folder_name = method_path+'\\\\'+f'{feature_importance_analysis}CSVMETA{meta_model}'\n",
    "            # get dataframe name\n",
    "            prediction_dataframe_name = folder_name+'\\\\'+f'PREDICTION{feature_importance_analysis}{meta_model}'\n",
    "            feature_importance_dataframe_name = folder_name+'\\\\'+f'RESULTS{feature_importance_analysis}{meta_model}'\n",
    "            additional_dataframe_name = folder_name+'\\\\'+f'ADDITIONAL{feature_importance_analysis}{meta_model}'\n",
    "            # get dataframe\n",
    "            prediction_dataframe = pd.read_csv(prediction_dataframe_name+'.csv')\n",
    "            feature_importance_dataframe = pd.read_csv(feature_importance_dataframe_name+'.csv')\n",
    "            try:\n",
    "                additional_dataframe = pd.read_csv(additional_dataframe_name+'.csv')\n",
    "            except:\n",
    "                additional_dataframe = pd.DataFrame()\n",
    "            # get the results of each\n",
    "            # normal prediction\n",
    "            seed_normal_prediction_std_dataframe = pd.DataFrame(columns=metrics_list+['SEED'])\n",
    "            for unique_seed in prediction_dataframe['SEED'].unique():\n",
    "                interested_dataframe = prediction_dataframe[prediction_dataframe['SEED']==unique_seed].drop(['ORDER'],axis='columns')\n",
    "                seed_std_list = []\n",
    "                for metric in metrics_list:\n",
    "                    single_seed_std = interested_dataframe[metric].std()\n",
    "                    seed_std_list.append(single_seed_std)\n",
    "                # append the unique seed\n",
    "                seed_std_list.append(unique_seed)\n",
    "                # put in dataframe\n",
    "                seed_normal_prediction_std_dataframe.loc[unique_seed] = seed_std_list\n",
    "            seed_normal_prediction_std_dataframe['METHOD'] = method_name   \n",
    "            seed_normal_prediction_std_dataframe['META'] = meta_model  \n",
    "            seed_normal_prediction_std_dataframe['FEATUREIMPORTANCE'] = feature_importance_analysis  \n",
    "            all_plot_normal_prediction_dataframe = pd.concat([seed_normal_prediction_std_dataframe,all_plot_normal_prediction_dataframe],axis='rows')\n",
    "            # feature importance\n",
    "            # change specifically for each feature importance\n",
    "            # there is 6 for 6 combinations\n",
    "            if feature_importance_analysis == 'FPI':\n",
    "                plot_fpi_dataframe = pd.DataFrame()\n",
    "                interested_column = feature_importance_dataframe.columns.drop(['TIME','SEED','ORDER','SHUFFLE']).to_list()\n",
    "                seed_feature_importance_std_dataframe = pd.DataFrame(columns=interested_column+['SEED'])\n",
    "                for unique_seed in feature_importance_dataframe['SEED'].unique():\n",
    "                    for unique_shuffle in feature_importance_dataframe['SHUFFLE'].unique():\n",
    "                        interested_dataframe = feature_importance_dataframe[(feature_importance_dataframe['SEED']==unique_seed) & (feature_importance_dataframe['SHUFFLE']==unique_shuffle)].drop(['ORDER'],axis='columns')\n",
    "                        temp_df = interested_dataframe[interested_column].std()\n",
    "                        temp_df['METHOD'] = method_name\n",
    "                        temp_df['META'] = meta_model\n",
    "                        temp_df['FEATUREIMPORTANCE'] = feature_importance_analysis\n",
    "                        temp_df['SHUFFLE'] = unique_shuffle\n",
    "                        temp_df['SEED'] = unique_seed\n",
    "                        temp_df = pd.DataFrame(temp_df).transpose()\n",
    "                        # print(temp_df)\n",
    "                        plot_fpi_dataframe = pd.concat([temp_df,plot_fpi_dataframe],axis='rows')\n",
    "                all_plot_fpi_prediction_dataframe = pd.concat([plot_fpi_dataframe,all_plot_fpi_prediction_dataframe],axis='rows')\n",
    "            elif feature_importance_analysis == 'PDP':\n",
    "                plot_pdp_dataframe = pd.DataFrame()\n",
    "                interested_column = feature_importance_dataframe.columns.drop(['SEED','ORDER','INDEX','PDPVALUES']).to_list()\n",
    "                seed_feature_importance_std_dataframe = pd.DataFrame(columns=interested_column+['SEED'])\n",
    "                for unique_seed in feature_importance_dataframe['SEED'].unique():\n",
    "                    for unique_index in feature_importance_dataframe['INDEX'].unique():\n",
    "                        interested_dataframe = feature_importance_dataframe[(feature_importance_dataframe['SEED']==unique_seed) & (feature_importance_dataframe['INDEX']==unique_index)].drop(['ORDER'],axis='columns')\n",
    "                        temp_df = interested_dataframe[interested_column].std()\n",
    "                        temp_df['METHOD'] = method_name\n",
    "                        temp_df['META'] = meta_model\n",
    "                        temp_df['FEATUREIMPORTANCE'] = feature_importance_analysis\n",
    "                        temp_df['INDEX'] = unique_index\n",
    "                        temp_df['SEED'] = unique_seed\n",
    "                        temp_df = pd.DataFrame(temp_df).transpose()\n",
    "                        # print(temp_df)\n",
    "                        plot_pdp_dataframe = pd.concat([temp_df,plot_pdp_dataframe],axis='rows')\n",
    "                all_plot_pdp_prediction_dataframe = pd.concat([plot_pdp_dataframe,all_plot_pdp_prediction_dataframe],axis='rows')\n",
    "            elif feature_importance_analysis == 'SHAP':\n",
    "                plot_shap_dataframe = pd.DataFrame()\n",
    "                interested_column = feature_importance_dataframe.columns.drop(['SEED','ORDER','INDEX']).to_list()\n",
    "                seed_feature_importance_std_dataframe = pd.DataFrame(columns=interested_column+['SEED'])\n",
    "                for unique_seed in feature_importance_dataframe['SEED'].unique():\n",
    "                    for unique_index in feature_importance_dataframe['INDEX'].unique():\n",
    "                        interested_dataframe = feature_importance_dataframe[(feature_importance_dataframe['SEED']==unique_seed) & (feature_importance_dataframe['INDEX']==unique_index)].drop(['ORDER'],axis='columns')\n",
    "                        temp_df = interested_dataframe[interested_column].std()\n",
    "                        temp_df['METHOD'] = method_name\n",
    "                        temp_df['META'] = meta_model\n",
    "                        temp_df['FEATUREIMPORTANCE'] = feature_importance_analysis\n",
    "                        temp_df['INDEX'] = unique_index\n",
    "                        temp_df['SEED'] = unique_seed\n",
    "                        temp_df = pd.DataFrame(temp_df).transpose()\n",
    "                        # print(temp_df)\n",
    "                        plot_shap_dataframe = pd.concat([temp_df,plot_shap_dataframe],axis='rows')\n",
    "                all_plot_shap_prediction_dataframe = pd.concat([plot_shap_dataframe,all_plot_shap_prediction_dataframe],axis='rows')\n",
    "\n",
    "# make new save folder\n",
    "pathlib.Path(f'std data').mkdir(parents=True,exist_ok=True)\n",
    "# only need 1 normal prediction because they are all the same for each feature importance analysis\n",
    "all_plot_normal_prediction_dataframe = all_plot_normal_prediction_dataframe[all_plot_normal_prediction_dataframe['FEATUREIMPORTANCE']=='FPI'].drop(['FEATUREIMPORTANCE'],axis='columns')\n",
    "all_plot_normal_prediction_dataframe.to_csv('std data\\\\stdnormalpredictiondataframe.csv',index=False)\n",
    "all_plot_fpi_prediction_dataframe.to_csv('std data\\\\stdfpipredictiondataframe.csv',index=False)\n",
    "all_plot_pdp_prediction_dataframe.to_csv('std data\\\\stdpdppredictiondataframe.csv',index=False)\n",
    "all_plot_shap_prediction_dataframe.to_csv('std data\\\\stdshappredictiondataframe.csv',index=False)\n",
    "# load in addtional information for plot\n",
    "pd.read_csv('\\\\'.join(path.split('\\\\')[:-1])+f'\\\\tuned model\\\\Ascending\\\\PDPCSVMETADT\\\\ADDITIONALPDPDT.csv').to_csv('std data\\\\additionalpdp.csv',index=False)\n",
    "pd.read_csv('\\\\'.join(path.split('\\\\')[:-1])+f'\\\\tuned model\\\\Ascending\\\\SHAPCSVMETADT\\\\ADDITIONALSHAPDT.csv').to_csv('std data\\\\additionalshap.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plot_normal_prediction_dataframe = pd.DataFrame()\n",
    "all_plot_fpi_prediction_dataframe = pd.DataFrame()\n",
    "all_plot_pdp_prediction_dataframe = pd.DataFrame()\n",
    "all_plot_shap_prediction_dataframe = pd.DataFrame()\n",
    "\n",
    "for method_name in method_name_list:\n",
    "    method_path = '\\\\'.join(path.split('\\\\')[:-1])+'\\\\'+'tuned model'+'\\\\'+method_name\n",
    "    for meta_model in meta_model_list:\n",
    "        for feature_importance_analysis in feature_importance_analysis_list:\n",
    "            folder_name = method_path+'\\\\'+f'{feature_importance_analysis}CSVMETA{meta_model}'\n",
    "            # get dataframe name\n",
    "            prediction_dataframe_name = folder_name+'\\\\'+f'PREDICTION{feature_importance_analysis}{meta_model}'\n",
    "            feature_importance_dataframe_name = folder_name+'\\\\'+f'RESULTS{feature_importance_analysis}{meta_model}'\n",
    "            additional_dataframe_name = folder_name+'\\\\'+f'ADDITIONAL{feature_importance_analysis}{meta_model}'\n",
    "            # get dataframe\n",
    "            prediction_dataframe = pd.read_csv(prediction_dataframe_name+'.csv')\n",
    "            feature_importance_dataframe = pd.read_csv(feature_importance_dataframe_name+'.csv')\n",
    "            try:\n",
    "                additional_dataframe = pd.read_csv(additional_dataframe_name+'.csv')\n",
    "            except:\n",
    "                additional_dataframe = pd.DataFrame()\n",
    "            # get the results of each\n",
    "            # normal prediction\n",
    "            seed_normal_prediction_mean_dataframe = pd.DataFrame(columns=metrics_list+['SEED'])\n",
    "            for unique_seed in prediction_dataframe['SEED'].unique():\n",
    "                interested_dataframe = prediction_dataframe[prediction_dataframe['SEED']==unique_seed].drop(['ORDER'],axis='columns')\n",
    "                seed_mean_list = []\n",
    "                for metric in metrics_list:\n",
    "                    single_seed_mean = interested_dataframe[metric].mean()\n",
    "                    seed_mean_list.append(single_seed_mean)\n",
    "                # append the unique seed\n",
    "                seed_mean_list.append(unique_seed)\n",
    "                # put in dataframe\n",
    "                seed_normal_prediction_mean_dataframe.loc[unique_seed] = seed_mean_list\n",
    "            seed_normal_prediction_mean_dataframe['METHOD'] = method_name   \n",
    "            seed_normal_prediction_mean_dataframe['META'] = meta_model  \n",
    "            seed_normal_prediction_mean_dataframe['FEATUREIMPORTANCE'] = feature_importance_analysis  \n",
    "            all_plot_normal_prediction_dataframe = pd.concat([seed_normal_prediction_mean_dataframe,all_plot_normal_prediction_dataframe],axis='rows')\n",
    "            # feature importance\n",
    "            # change specifically for each feature importance\n",
    "            # there is 6 for 6 combinations\n",
    "            if feature_importance_analysis == 'FPI':\n",
    "                plot_fpi_dataframe = pd.DataFrame()\n",
    "                interested_column = feature_importance_dataframe.columns.drop(['TIME','SEED','ORDER','SHUFFLE']).to_list()\n",
    "                seed_feature_importance_mean_dataframe = pd.DataFrame(columns=interested_column+['SEED'])\n",
    "                for unique_seed in feature_importance_dataframe['SEED'].unique():\n",
    "                    for unique_shuffle in feature_importance_dataframe['SHUFFLE'].unique():\n",
    "                        interested_dataframe = feature_importance_dataframe[(feature_importance_dataframe['SEED']==unique_seed) & (feature_importance_dataframe['SHUFFLE']==unique_shuffle)].drop(['ORDER'],axis='columns')\n",
    "                        temp_df = interested_dataframe[interested_column].mean()\n",
    "                        temp_df['METHOD'] = method_name\n",
    "                        temp_df['META'] = meta_model\n",
    "                        temp_df['FEATUREIMPORTANCE'] = feature_importance_analysis\n",
    "                        temp_df['SHUFFLE'] = unique_shuffle\n",
    "                        temp_df['SEED'] = unique_seed\n",
    "                        temp_df = pd.DataFrame(temp_df).transpose()\n",
    "                        # print(temp_df)\n",
    "                        plot_fpi_dataframe = pd.concat([temp_df,plot_fpi_dataframe],axis='rows')\n",
    "                all_plot_fpi_prediction_dataframe = pd.concat([plot_fpi_dataframe,all_plot_fpi_prediction_dataframe],axis='rows')\n",
    "            elif feature_importance_analysis == 'PDP':\n",
    "                plot_pdp_dataframe = pd.DataFrame()\n",
    "                interested_column = feature_importance_dataframe.columns.drop(['SEED','ORDER','INDEX','PDPVALUES']).to_list()\n",
    "                seed_feature_importance_mean_dataframe = pd.DataFrame(columns=interested_column+['SEED'])\n",
    "                for unique_seed in feature_importance_dataframe['SEED'].unique():\n",
    "                    for unique_index in feature_importance_dataframe['INDEX'].unique():\n",
    "                        interested_dataframe = feature_importance_dataframe[(feature_importance_dataframe['SEED']==unique_seed) & (feature_importance_dataframe['INDEX']==unique_index)].drop(['ORDER'],axis='columns')\n",
    "                        temp_df = interested_dataframe[interested_column].mean()\n",
    "                        temp_df['METHOD'] = method_name\n",
    "                        temp_df['META'] = meta_model\n",
    "                        temp_df['FEATUREIMPORTANCE'] = feature_importance_analysis\n",
    "                        temp_df['INDEX'] = unique_index\n",
    "                        temp_df['SEED'] = unique_seed\n",
    "                        temp_df = pd.DataFrame(temp_df).transpose()\n",
    "                        # print(temp_df)\n",
    "                        plot_pdp_dataframe = pd.concat([temp_df,plot_pdp_dataframe],axis='rows')\n",
    "                all_plot_pdp_prediction_dataframe = pd.concat([plot_pdp_dataframe,all_plot_pdp_prediction_dataframe],axis='rows')\n",
    "            elif feature_importance_analysis == 'SHAP':\n",
    "                plot_shap_dataframe = pd.DataFrame()\n",
    "                interested_column = feature_importance_dataframe.columns.drop(['SEED','ORDER','INDEX']).to_list()\n",
    "                seed_feature_importance_mean_dataframe = pd.DataFrame(columns=interested_column+['SEED'])\n",
    "                for unique_seed in feature_importance_dataframe['SEED'].unique():\n",
    "                    for unique_index in feature_importance_dataframe['INDEX'].unique():\n",
    "                        interested_dataframe = feature_importance_dataframe[(feature_importance_dataframe['SEED']==unique_seed) & (feature_importance_dataframe['INDEX']==unique_index)].drop(['ORDER'],axis='columns')\n",
    "                        temp_df = interested_dataframe[interested_column].mean()\n",
    "                        temp_df['METHOD'] = method_name\n",
    "                        temp_df['META'] = meta_model\n",
    "                        temp_df['FEATUREIMPORTANCE'] = feature_importance_analysis\n",
    "                        temp_df['INDEX'] = unique_index\n",
    "                        temp_df['SEED'] = unique_seed\n",
    "                        temp_df = pd.DataFrame(temp_df).transpose()\n",
    "                        # print(temp_df)\n",
    "                        plot_shap_dataframe = pd.concat([temp_df,plot_shap_dataframe],axis='rows')\n",
    "                all_plot_shap_prediction_dataframe = pd.concat([plot_shap_dataframe,all_plot_shap_prediction_dataframe],axis='rows')\n",
    "\n",
    "# make new save folder\n",
    "pathlib.Path(f'mean data').mkdir(parents=True,exist_ok=True)\n",
    "# only need 1 normal prediction because they are all the same for each feature importance analysis\n",
    "all_plot_normal_prediction_dataframe = all_plot_normal_prediction_dataframe[all_plot_normal_prediction_dataframe['FEATUREIMPORTANCE']=='FPI'].drop(['FEATUREIMPORTANCE'],axis='columns')\n",
    "all_plot_normal_prediction_dataframe.to_csv('mean data\\\\meannormalpredictiondataframe.csv',index=False)\n",
    "all_plot_fpi_prediction_dataframe.to_csv('mean data\\\\meanfpipredictiondataframe.csv',index=False)\n",
    "all_plot_pdp_prediction_dataframe.to_csv('mean data\\\\meanpdppredictiondataframe.csv',index=False)\n",
    "all_plot_shap_prediction_dataframe.to_csv('mean data\\\\meanshappredictiondataframe.csv',index=False)\n",
    "# load in addtional information for plot\n",
    "pd.read_csv('\\\\'.join(path.split('\\\\')[:-1])+f'\\\\tuned model\\\\Ascending\\\\PDPCSVMETADT\\\\ADDITIONALPDPDT.csv').to_csv('mean data\\\\additionalpdp.csv',index=False)\n",
    "pd.read_csv('\\\\'.join(path.split('\\\\')[:-1])+f'\\\\tuned model\\\\Ascending\\\\SHAPCSVMETADT\\\\ADDITIONALSHAPDT.csv').to_csv('mean data\\\\additionalshap.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
