{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://journals.lww.com/epidem/FullText/2019/07000/Can_Hyperparameter_Tuning_Improve_the_Performance.9.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuning is only done for the base model, as the meta model needs to learn from the kfold set and tuning would cause possible data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the optimization is similar to the paper https://www.sciencedirect.com/science/article/pii/S2666827022000020 where they optimized the base models on k-fold, then they aggregated the tuned parameters and used it for the refit base models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 steps\n",
    "# 1) k-fold base models are tuned and trained for best parameters -> obtain training set to train meta model on tuned base models\n",
    "# 2) retrained base models are fitted with their best parameters and combined with meta model -> final stacking model\n",
    "# helps to prevent data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the k-fold models find their best parameters for the k-fold dataset, the meta model finds the best parameters for the meta training set, and the refit model finds the best parameters for the whole train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sorting the whole column, not just rows?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import sklearn\n",
    "import shap \n",
    "import time\n",
    "import math\n",
    "import sys \n",
    "import pathlib\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SA</th>\n",
       "      <th>DG</th>\n",
       "      <th>%N</th>\n",
       "      <th>%O</th>\n",
       "      <th>%S</th>\n",
       "      <th>%P</th>\n",
       "      <th>%B</th>\n",
       "      <th>CD</th>\n",
       "      <th>CAP</th>\n",
       "      <th>CONC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>343.5</td>\n",
       "      <td>0.84</td>\n",
       "      <td>2.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>292.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>784.0</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784.0</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>784.0</td>\n",
       "      <td>1.06</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>784.0</td>\n",
       "      <td>1.06</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>571.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.50</td>\n",
       "      <td>11.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>544.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.47</td>\n",
       "      <td>11.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>373.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.36</td>\n",
       "      <td>15.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>460.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.84</td>\n",
       "      <td>14.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>413.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.30</td>\n",
       "      <td>10.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1089 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SA    DG    %N     %O  %S   %P  %B    CD    CAP  CONC\n",
       "0     343.5  0.84  2.30    NaN NaN  4.5 NaN   0.5  292.0   6.0\n",
       "1     784.0  1.05  3.50   4.10 NaN  NaN NaN   0.5   98.0   1.0\n",
       "2     784.0  1.05  3.50   4.10 NaN  NaN NaN  20.0   58.0   1.0\n",
       "3     784.0  1.06  4.50   4.40 NaN  NaN NaN   0.5  104.0   1.0\n",
       "4     784.0  1.06  4.50   4.40 NaN  NaN NaN  20.0   49.0   1.0\n",
       "...     ...   ...   ...    ...  ..  ...  ..   ...    ...   ...\n",
       "1084  571.0   NaN  9.50  11.81 NaN  NaN NaN   1.0  239.0   6.0\n",
       "1085  544.0   NaN  8.47  11.77 NaN  NaN NaN   1.0  250.0   6.0\n",
       "1086  373.0   NaN  6.36  15.91 NaN  NaN NaN   1.0  193.0   6.0\n",
       "1087  460.0   NaN  7.84  14.40 NaN  NaN NaN   1.0  222.0   6.0\n",
       "1088  413.0   NaN  7.30  10.73 NaN  NaN NaN   1.0  207.0   6.0\n",
       "\n",
       "[1089 rows x 10 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.abspath('')\n",
    "os.chdir(f'{path}')\n",
    "\n",
    "original_dataframe_path = '\\\\'.join(path.split('\\\\')[:-1])+'\\\\data\\\\Supercapacitor V2.csv'\n",
    "original_dataframe = pd.read_csv(original_dataframe_path)\n",
    "\n",
    "target_name = 'CAP'\n",
    "original_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Main_Stacking_Process:\n",
    "\n",
    "    # --- MAIN PROCESS LOOP\n",
    "\n",
    "    # initialize process\n",
    "    def __init__(self,base_model_names,base_model_classes,stack_meta_model_name,stack_meta_model_class,method):\n",
    "\n",
    "        self.method = method\n",
    "        self.dataframe = original_dataframe.copy()\n",
    "        self.base_model_names = base_model_names\n",
    "        self.base_model_classes = base_model_classes\n",
    "        self.stack_meta_model_name = stack_meta_model_name\n",
    "        self.stack_meta_model_class = stack_meta_model_class\n",
    "        self.depth = len(self.base_model_names)\n",
    "        self.dt_parameters = dict(criterion=['squared_error','friedman_mse','absolute_error','poisson'])\n",
    "        self.knn_parameters = dict(n_neighbors=[5,20],weights=['uniform','distance'])\n",
    "        self.lin_parameters = dict() # doesn't need to be tuned\n",
    "        self.parameters_dict = {\n",
    "            'LIN':self.lin_parameters,\n",
    "            'DT':self.dt_parameters,\n",
    "            'KNN':self.knn_parameters,\n",
    "        }\n",
    "\n",
    "        print('Start')\n",
    "        print(f'META: {self.stack_meta_model_name}')\n",
    "        print(f'MODELS: {self.base_model_names}')\n",
    "\n",
    "        # loop through all techniques\n",
    "        feature_importance_techniques_list = ['FPI','PDP','SHAP']\n",
    "        for feature_importance_technique in feature_importance_techniques_list:\n",
    "            print(feature_importance_technique)\n",
    "            self.process(feature_importance_technique)\n",
    "\n",
    "    # find model order/combinations\n",
    "    def find_model_combination(self,stack_model_tuple): \n",
    "        combination_tuple = []\n",
    "        def recursion(depth,current_tuple=[]):\n",
    "            if depth == 0:\n",
    "                combination = current_tuple\n",
    "                combination_tuple.append(combination)\n",
    "            else:\n",
    "                for model in stack_model_tuple: # for every model in the model tuple\n",
    "                    recursion(depth-1,current_tuple+[model]) # recursion and add model to tuple\n",
    "        recursion(self.depth)\n",
    "        return combination_tuple\n",
    "\n",
    "    # main training loop, each for SHAP, PDP, and FPI\n",
    "    def process(self,feature_importance_technique):\n",
    "        # create file to store dataframes\n",
    "        pathlib.Path(f'{self.method}\\\\{feature_importance_technique}CSVMETA{self.stack_meta_model_name}').mkdir(parents=True,exist_ok=True)\n",
    "        # get order of base models\n",
    "        combination_df_columns = ['Model'+str(x) for x in range(self.depth)]\n",
    "        combination_tuple = self.find_model_combination(self.base_model_names)\n",
    "        combination_df = pd.DataFrame(combination_tuple,columns=combination_df_columns)\n",
    "        combination_df = combination_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "        for row in combination_df.index:\n",
    "            if len(combination_df.loc[row].values) != len(set(combination_df.loc[row].values)):\n",
    "                combination_df = combination_df.drop(row)\n",
    "        combination_df = combination_df.reset_index(drop=True)\n",
    "        # initialize feature importance technique dataframes\n",
    "        # stores all the results of model prediction to compare\n",
    "        self.all_prediction_results_df = pd.DataFrame() \n",
    "        self.all_feature_importance_results_df = pd.DataFrame()\n",
    "        self.all_additional_plotting_value_df = pd.DataFrame()\n",
    "        # train models on each order\n",
    "        for combination_df_index in combination_df.index:\n",
    "            self.combination_name_tuple = tuple(combination_df.loc[combination_df_index].values)\n",
    "            self.combination_class_tuple = []\n",
    "            self.complete_order_name = ''\n",
    "            for stack_model in self.combination_name_tuple:\n",
    "                self.combination_class_tuple.append(self.base_model_classes[stack_model])\n",
    "                self.complete_order_name = self.complete_order_name + stack_model + '-'\n",
    "            self.combination_class_tuple = tuple(self.combination_class_tuple)\n",
    "            # create dataframe to store all seed results\n",
    "            self.seed_prediction_results_df = pd.DataFrame()\n",
    "            self.seed_feature_importance_results_df = pd.DataFrame()\n",
    "            self.seed_additional_plotting_value_df = pd.DataFrame()\n",
    "            # train in loop\n",
    "            for random_seed in range(5):\n",
    "                # process\n",
    "                self.random_seed = random_seed \n",
    "                self.random_state = np.random.RandomState(self.random_seed)\n",
    "                self.fill_missing_values()\n",
    "                self.split_train_and_test()\n",
    "                self.impute_data()\n",
    "                self.scale_data()\n",
    "                self.k_fold()\n",
    "                self.fit()\n",
    "                self.predict()\n",
    "                if feature_importance_technique == 'FPI':\n",
    "                    self.FPI_prediction_function()\n",
    "                elif feature_importance_technique == 'PDP':\n",
    "                    self.PDP_prediction_function()\n",
    "                elif feature_importance_technique == 'SHAP':\n",
    "                    self.SHAP_prediction_function()\n",
    "                else:\n",
    "                    print('error')\n",
    "                # feature importance process\n",
    "                self.seed_prediction_results_df = pd.concat([self.temp_normal_prediction_results_df,self.seed_prediction_results_df],axis='rows')\n",
    "                self.seed_feature_importance_results_df = pd.concat([self.temp_feature_importance_results_df,self.seed_feature_importance_results_df],axis='rows')\n",
    "                self.seed_additional_plotting_value_df = pd.concat([self.temp_additional_plotting_value_df,self.seed_additional_plotting_value_df],axis='rows')  \n",
    "            # accumulate for all seeds\n",
    "            self.all_prediction_results_df = pd.concat([self.seed_prediction_results_df,self.all_prediction_results_df],axis='rows') \n",
    "            self.all_feature_importance_results_df = pd.concat([self.seed_feature_importance_results_df,self.all_feature_importance_results_df],axis='rows')\n",
    "            self.all_additional_plotting_value_df = pd.concat([self.seed_additional_plotting_value_df,self.all_additional_plotting_value_df],axis='rows')\n",
    "        # reset index\n",
    "        self.all_prediction_results_df = self.all_prediction_results_df.reset_index(drop=True)\n",
    "        self.all_feature_importance_results_df = self.all_feature_importance_results_df.reset_index(drop=True)\n",
    "        self.all_additional_plotting_value_df = self.all_additional_plotting_value_df.reset_index(drop=True)\n",
    "        # save results\n",
    "        self.all_prediction_results_df.to_csv(f'{self.method}\\\\{feature_importance_technique}CSVMETA{self.stack_meta_model_name}\\\\PREDICTION{feature_importance_technique}{self.stack_meta_model_name}.csv',index=False)\n",
    "        self.all_feature_importance_results_df.to_csv(f'{self.method}\\\\{feature_importance_technique}CSVMETA{self.stack_meta_model_name}\\\\RESULTS{feature_importance_technique}{self.stack_meta_model_name}.csv',index=False)        \n",
    "        self.all_additional_plotting_value_df.to_csv(f'{self.method}\\\\{feature_importance_technique}CSVMETA{self.stack_meta_model_name}\\\\ADDITIONAL{feature_importance_technique}{self.stack_meta_model_name}.csv',index=False)        \n",
    "\n",
    "    # --- MODEL FUNCTIONS\n",
    "\n",
    "    # fill missing values\n",
    "    def fill_missing_values(self):\n",
    "        self.dataframe = self.dataframe.fillna(0)\n",
    "\n",
    "    # split into train/test\n",
    "    def split_train_and_test(self): \n",
    "        # split 70/30 train/test\n",
    "        self.dataframe_feature = self.dataframe.drop([target_name],axis='columns')\n",
    "        self.dataframe_target = self.dataframe[[target_name]]      \n",
    "        self.dataframe_train_feature, self.dataframe_test_feature, self.dataframe_train_target, self.dataframe_test_target = train_test_split(self.dataframe_feature,self.dataframe_target,test_size=0.3,random_state=self.random_state)\n",
    "\n",
    "    # impute missing data, being careful of data leakage\n",
    "    def impute_data(self):\n",
    "        # get splits for imputers, non-imputes, and training \n",
    "        dataframe_impute_missing = self.dataframe_train_feature[self.dataframe_train_feature['DG'] == 0].drop(['CD','CONC'],axis='columns')\n",
    "        dataframe_impute_non_missing = self.dataframe_train_feature[self.dataframe_train_feature['DG'] != 0].drop(['CD','CONC'],axis='columns')\n",
    "        dataframe_unused_train_set = self.dataframe_train_feature[['CD','CONC']]\n",
    "        dataframe_KNN_imputor_train_feature = dataframe_impute_non_missing.drop(['DG'],axis='columns')\n",
    "        dataframe_KNN_imputor_train_target = dataframe_impute_non_missing['DG']\n",
    "        # train KNN imputor based on train split\n",
    "        KNN_imputor = KNeighborsRegressor(\n",
    "            n_neighbors=3,\n",
    "            weights='distance'\n",
    "        )\n",
    "        KNN_imputor.fit(dataframe_KNN_imputor_train_feature,dataframe_KNN_imputor_train_target)\n",
    "        # predict for missing data in train set\n",
    "        dataframe_impute_missing_feature = dataframe_impute_missing.drop(['DG'],axis='columns')\n",
    "        imputation_index = dataframe_impute_missing_feature.index\n",
    "        imputation_prediction = pd.DataFrame(KNN_imputor.predict(dataframe_impute_missing_feature),columns=['DG'])\n",
    "        imputation_prediction = imputation_prediction.set_index(imputation_index)\n",
    "        imputed_feature = pd.concat([imputation_prediction,dataframe_impute_missing_feature],axis='columns')\n",
    "        imputed_and_non_missing_dataframe = pd.concat([imputed_feature,dataframe_impute_non_missing],axis='rows')\n",
    "        old_index = self.dataframe_train_feature.index\n",
    "        imputed_and_non_missing_dataframe = pd.concat([imputed_feature,dataframe_impute_non_missing],axis='rows').reindex(old_index)\n",
    "        imputed_and_non_missing_dataframe = pd.concat([imputed_and_non_missing_dataframe,dataframe_unused_train_set],axis='columns')\n",
    "        self.dataframe_train_feature = imputed_and_non_missing_dataframe\n",
    "        self.dataframe_train_target = self.dataframe_train_target\n",
    "        # predict for missing data in test set #\n",
    "        dataframe_impute_missing_test = self.dataframe_test_feature[self.dataframe_test_feature['DG'] == 0].drop(['CD','CONC'],axis='columns')\n",
    "        dataframe_impute_non_missing_test = self.dataframe_test_feature[self.dataframe_test_feature['DG'] != 0].drop(['CD','CONC'],axis='columns')\n",
    "        dataframe_unused_test_set = self.dataframe_test_feature[['CD','CONC']]\n",
    "        dataframe_impute_missing_feature_test = dataframe_impute_missing_test.drop(['DG'],axis='columns')\n",
    "        imputation_index_test = dataframe_impute_missing_feature_test.index\n",
    "        imputation_prediction_test = pd.DataFrame(KNN_imputor.predict(dataframe_impute_missing_feature_test),columns=['DG'])\n",
    "        imputation_prediction_test = imputation_prediction_test.set_index(imputation_index_test)\n",
    "        imputed_feature_test = pd.concat([imputation_prediction_test,dataframe_impute_missing_feature_test],axis='columns')\n",
    "        imputed_and_non_missing_dataframe_test = pd.concat([imputed_feature_test,dataframe_impute_non_missing_test],axis='rows')\n",
    "        old_index = self.dataframe_test_feature.index\n",
    "        imputed_and_non_missing_dataframe_test = pd.concat([imputed_feature_test,dataframe_impute_non_missing_test],axis='rows').reindex(old_index)\n",
    "        imputed_and_non_missing_dataframe_test = pd.concat([imputed_and_non_missing_dataframe_test,dataframe_unused_test_set],axis='columns')\n",
    "        self.dataframe_test_feature = imputed_and_non_missing_dataframe_test\n",
    "        self.dataframe_test_target = self.dataframe_test_target\n",
    "\n",
    "    # scale data based on train set\n",
    "    def scale_data(self):\n",
    "        # keep scale value to rescale later\n",
    "        self.feature_scale_dict = {}\n",
    "        self.target_scale_dict = {}\n",
    "        # scale based on train set for feature #\n",
    "        for feature_column in self.dataframe_train_feature:\n",
    "            self.feature_column_max = self.dataframe_train_feature[feature_column].max()\n",
    "            self.feature_column_min = self.dataframe_train_feature[feature_column].min()\n",
    "            self.feature_scale_dict[f'MIN_{feature_column}'] = self.feature_column_min\n",
    "            self.feature_scale_dict[f'MAX_{feature_column}'] = self.feature_column_max            \n",
    "            self.dataframe_train_feature[feature_column] = self.dataframe_train_feature[feature_column].apply(lambda x: (x-self.feature_column_min)/(self.feature_column_max-self.feature_column_min))\n",
    "            self.dataframe_test_feature[feature_column] = self.dataframe_test_feature[feature_column].apply(lambda x: (x-self.feature_column_min)/(self.feature_column_max-self.feature_column_min))\n",
    "        # scale based on train set for target #\n",
    "        for target_column in self.dataframe_train_target:\n",
    "            self.target_column_max = self.dataframe_train_target[target_column].max()\n",
    "            self.target_column_min = self.dataframe_train_target[target_column].min()\n",
    "            self.target_scale_dict[f'MIN_{target_column}'] = self.target_column_min\n",
    "            self.target_scale_dict[f'MAX_{target_column}'] = self.target_column_max \n",
    "            self.dataframe_train_target[target_column] = self.dataframe_train_target[target_column].apply(lambda x: (x-self.target_column_min)/(self.target_column_max-self.target_column_min))\n",
    "            self.dataframe_test_target[target_column] = self.dataframe_test_target[target_column].apply(lambda x: (x-self.target_column_min)/(self.target_column_max-self.target_column_min))\n",
    "    \n",
    "    # determine number of folds used when training meta model\n",
    "    def k_fold(self):\n",
    "        self.number_of_folds = 5\n",
    "        kfold_holder_for_training = KFold(n_splits=self.number_of_folds,random_state=None,shuffle=False)\n",
    "        self.splits_for_training = kfold_holder_for_training.split(self.dataframe_train_feature)\n",
    "\n",
    "    # train the model, fit function\n",
    "    def fit(self):\n",
    "        # keep track of time\n",
    "        self.train_start_time = time.time()\n",
    "        # get each model used\n",
    "        self.model_dict = {}\n",
    "        for model in self.combination_name_tuple:\n",
    "            self.model_dict[model] = {}\n",
    "        # accumulate prediction set\n",
    "        self.prediction_list = []\n",
    "        self.META_hold_out_prediction_list = []\n",
    "        # meta model training process\n",
    "        for count_for_train, (train_index_for_validation,test_index_for_validation) in enumerate(self.splits_for_training):\n",
    "            # splits\n",
    "            kfold_train_feature_df = self.dataframe_train_feature.iloc[train_index_for_validation]\n",
    "            kfold_train_target_df = self.dataframe_train_target.iloc[train_index_for_validation]\n",
    "            kfold_test_feature_df = self.dataframe_train_feature.iloc[test_index_for_validation]\n",
    "            kfold_test_target_df = self.dataframe_train_target.iloc[test_index_for_validation]\n",
    "            # train model\n",
    "            for count,model_name in enumerate(self.combination_name_tuple):\n",
    "                self.model_dict[model_name] = {}\n",
    "                # give the model random state of 0 if possible\n",
    "                # check for best parameter, try with random state 0\n",
    "                try:\n",
    "                    estimator = self.combination_class_tuple[count]()\n",
    "                    param_grid = self.parameters_dict[model_name].copy()\n",
    "                    param_grid['random_state'] = [np.random.RandomState(0)]\n",
    "                    k_fold_grid_search = sklearn.model_selection.GridSearchCV(estimator=estimator,param_grid=param_grid)\n",
    "                    k_fold_grid_search.fit(kfold_train_feature_df,kfold_train_target_df[['CAP']]) \n",
    "                except:\n",
    "                    estimator = self.combination_class_tuple[count]()\n",
    "                    param_grid = self.parameters_dict[model_name].copy()\n",
    "                    k_fold_grid_search = sklearn.model_selection.GridSearchCV(estimator=estimator,param_grid=param_grid)\n",
    "                    k_fold_grid_search.fit(kfold_train_feature_df,kfold_train_target_df[['CAP']]) \n",
    "                model = self.combination_class_tuple[count](\n",
    "                    **k_fold_grid_search.best_params_\n",
    "                )\n",
    "                model.fit(kfold_train_feature_df,kfold_train_target_df[['CAP']])\n",
    "                model_prediction_for_training_meta_model = model.predict(kfold_test_feature_df)\n",
    "                # accumulate results of model\n",
    "                self.model_dict[model_name][count_for_train] = {}\n",
    "                self.model_dict[model_name][count_for_train]['model'] = model\n",
    "                self.model_dict[model_name][count_for_train]['meta_feature'] = model_prediction_for_training_meta_model                \n",
    "            # stacking\n",
    "            META_hold_out_prediction_df = pd.DataFrame()\n",
    "            for count,key in enumerate(self.model_dict):\n",
    "                meta_feature_df = pd.DataFrame(self.model_dict[key][count_for_train]['meta_feature'],columns=[f'MODEL {count}'])\n",
    "                META_hold_out_prediction_df = pd.concat([META_hold_out_prediction_df,meta_feature_df],axis='columns')\n",
    "            self.META_hold_out_prediction_list.append(META_hold_out_prediction_df)\n",
    "        # meta model training set\n",
    "        self.META_train_df = pd.concat(self.META_hold_out_prediction_list,axis='rows').reset_index(drop=True)\n",
    "        # meta model\n",
    "        # check for best parameter, try with random state 0\n",
    "        try:\n",
    "            estimator = self.stack_meta_model_class()\n",
    "            param_grid = self.parameters_dict[self.stack_meta_model_name].copy()\n",
    "            param_grid['random_state'] = [np.random.RandomState(0)]\n",
    "            meta_model_grid_search = sklearn.model_selection.GridSearchCV(estimator=estimator,param_grid=param_grid)\n",
    "            meta_model_grid_search.fit(self.META_train_df.values,self.dataframe_train_target.values.ravel()) \n",
    "        except:\n",
    "            estimator = self.stack_meta_model_class()\n",
    "            param_grid = self.parameters_dict[self.stack_meta_model_name].copy()\n",
    "            meta_model_grid_search = sklearn.model_selection.GridSearchCV(estimator=estimator,param_grid=param_grid)\n",
    "            meta_model_grid_search.fit(self.META_train_df.values,self.dataframe_train_target.values.ravel()) \n",
    "        META_model = self.stack_meta_model_class(\n",
    "            **meta_model_grid_search.best_params_\n",
    "        )\n",
    "        META_model.fit(self.META_train_df.values,self.dataframe_train_target.values.ravel())\n",
    "        self.META_model = META_model\n",
    "        # refit base models\n",
    "        self.refit_base_model_dict = {}\n",
    "        for count,model_name in enumerate(self.combination_name_tuple):\n",
    "            # check for best parameter, try with random state 0\n",
    "            try:\n",
    "                estimator = self.combination_class_tuple[count]()\n",
    "                param_grid = self.parameters_dict[model_name].copy()\n",
    "                param_grid['random_state'] = [np.random.RandomState(0)]\n",
    "                refit_grid_search = sklearn.model_selection.GridSearchCV(estimator=estimator,param_grid=param_grid)\n",
    "                refit_grid_search.fit(self.META_train_df.values,self.dataframe_train_target.values.ravel()) \n",
    "            except:\n",
    "                estimator = self.combination_class_tuple[count]()\n",
    "                param_grid = self.parameters_dict[model_name].copy()\n",
    "                refit_grid_search = sklearn.model_selection.GridSearchCV(estimator=estimator,param_grid=param_grid)\n",
    "                refit_grid_search.fit(self.META_train_df.values,self.dataframe_train_target.values.ravel()) \n",
    "            self.refit_base_model_dict[f'refit{model_name}'] = {}\n",
    "            model = self.combination_class_tuple[count](\n",
    "                **refit_grid_search.best_params_\n",
    "            )\n",
    "            model.fit(self.dataframe_train_feature.values,self.dataframe_train_target.values.ravel())\n",
    "            self.refit_base_model_dict[f'refit{model_name}']['model'] = model\n",
    "            # code name\n",
    "            self.refit_base_model_dict[f'refit{model_name}']['code_name'] = f'MODEL {count}'\n",
    "        self.train_end_time = time.time()\n",
    "\n",
    "    # stacking model prediction function\n",
    "    def model_prediction(self,features):\n",
    "        META_prediction_df = pd.DataFrame()\n",
    "        for count,refit_model in enumerate(self.refit_base_model_dict):\n",
    "            try:\n",
    "                refit_model_prediction = pd.DataFrame(self.refit_base_model_dict[refit_model]['model'].predict(features.values),columns=[self.refit_base_model_dict[refit_model]['code_name']])\n",
    "            except:\n",
    "                refit_model_prediction = pd.DataFrame(self.refit_base_model_dict[refit_model]['model'].predict(features),columns=[self.refit_base_model_dict[refit_model]['code_name']])\n",
    "            META_prediction_df = pd.concat([META_prediction_df,refit_model_prediction],axis='columns')\n",
    "        try:\n",
    "            self.META_prediction = self.META_model.predict(META_prediction_df.values)\n",
    "        except:\n",
    "            self.META_prediction = self.META_model.predict(META_prediction_df)\n",
    "        return self.META_prediction     \n",
    "\n",
    "    # predict and compare to get error\n",
    "    def predict(self):\n",
    "        # rescale from model prediction \n",
    "        self.prediction = self.model_prediction(self.dataframe_test_feature)\n",
    "        self.prediction = pd.DataFrame(self.prediction,columns=['CAP'])\n",
    "        self.prediction = self.prediction.set_index(self.dataframe_test_target.index)\n",
    "        self.real = self.dataframe_test_target[[target_name]].loc[self.dataframe_test_target.index]\n",
    "        self.prediction = (self.prediction * (self.target_scale_dict['MAX_CAP'] - self.target_scale_dict['MIN_CAP'])) + self.target_scale_dict['MIN_CAP']\n",
    "        self.real = (self.real * (self.target_scale_dict['MAX_CAP'] - self.target_scale_dict['MIN_CAP'])) + self.target_scale_dict['MIN_CAP']\n",
    "        # all metrics\n",
    "        self.y_mean = self.real.mean().values[0]\n",
    "        self.MAE = abs(self.real - self.prediction).mean().values[0] # mean absolute error\n",
    "        self.MSE = ((self.real - self.prediction)**2).mean().values[0] # mean squared error\n",
    "        self.RMSE = math.sqrt(((self.real - self.prediction)**2).mean().values[0]) # root mean squared error\n",
    "        self.R2 = 1 - (((self.real - self.prediction)**2).sum().values[0]/((self.real - self.y_mean)**2).sum().values[0]) # R2\n",
    "        self.MAPE = abs((self.real - self.prediction)/self.real).mean().values[0] # mean absolute percentage error  \n",
    "        # export results\n",
    "        self.temp_normal_prediction_results_df = pd.DataFrame(\n",
    "            [[self.MAE,self.MSE,self.RMSE,self.R2,self.MAPE,(self.train_end_time-self.train_start_time)/60,self.random_seed,self.complete_order_name]],\n",
    "            columns=['MAE','MSE','RMSE','R2','MAPE','TIME','SEED','ORDER']\n",
    "        )\n",
    "\n",
    "    # shap\n",
    "    def SHAP_prediction_function(self):\n",
    "        # need 3 things 1) prediction function 2) training set 3) validation set\n",
    "        explainer = shap.KernelExplainer(self.model_prediction,self.dataframe_train_feature.head(5))\n",
    "        shap_values = explainer.shap_values(self.dataframe_test_feature.head(5),silent=True) # silent diables logging\n",
    "        # shap results\n",
    "        self.shap_df = pd.DataFrame(shap_values,columns=self.dataframe_train_feature.columns)\n",
    "        self.shap_df['SEED'] = self.random_seed\n",
    "        self.shap_df['ORDER'] = self.complete_order_name\n",
    "        self.shap_df['INDEX'] = self.dataframe_test_feature.head(5).index\n",
    "        # original values for plot\n",
    "        self.original_feature_value_df = pd.DataFrame(self.dataframe_test_feature.head(5),columns=self.dataframe_test_feature.head(5).columns)\n",
    "        for feature_column in self.original_feature_value_df:\n",
    "            self.original_feature_value_df[feature_column] = self.original_feature_value_df[feature_column].apply(lambda x: (x * (self.feature_scale_dict[f'MAX_{feature_column}'] - self.feature_scale_dict[f'MIN_{feature_column}'])) + self.feature_scale_dict[f'MIN_{feature_column}'])\n",
    "        self.original_feature_value_df['SEED'] = self.random_seed\n",
    "        self.original_feature_value_df['ORDER'] = self.complete_order_name\n",
    "        self.original_feature_value_df['INDEX'] = self.original_feature_value_df.index\n",
    "        # export results\n",
    "        self.temp_feature_importance_results_df = self.shap_df\n",
    "        self.temp_additional_plotting_value_df = self.original_feature_value_df\n",
    "\n",
    "    # pdp\n",
    "    def PDP_prediction_function(self):\n",
    "        self.PDP_prediction_df = pd.DataFrame()\n",
    "        # get the mean max and min valyes of the training features to plot later\n",
    "        mean_value_series = self.dataframe_train_feature.mean()\n",
    "        max_value_series = self.dataframe_train_feature.max()\n",
    "        min_value_series = self.dataframe_train_feature.min()\n",
    "        mean_value_df = pd.DataFrame(columns=mean_value_series.index)\n",
    "        mean_value_df.loc[0] = 0.5 # mean\n",
    "        mean_value_df.loc[1] = 1 # max\n",
    "        mean_value_df.loc[2] = 0 # min\n",
    "        mean_value_df.index = ['MEAN','MAX','MIN']\n",
    "        column_list = mean_value_df.columns\n",
    "        export_mean_value_df = mean_value_df.copy()\n",
    "        # the pdp process\n",
    "        all_PDP_prediction_results = pd.DataFrame()\n",
    "        for column in column_list:\n",
    "            # get intervals of 10% from max to min to put in PDP\n",
    "            column_values_list = np.arange(mean_value_df[column]['MIN'],mean_value_df[column]['MAX']+(abs(mean_value_df[column]['MAX']-mean_value_df[column]['MIN'])/10),abs(mean_value_df[column]['MAX']-mean_value_df[column]['MIN'])/10)\n",
    "            column_values_df = pd.DataFrame()\n",
    "            for column_values in column_values_list:\n",
    "                temp_df = pd.DataFrame([mean_value_df.loc['MEAN']],columns=mean_value_df.columns)\n",
    "                temp_df[column] = column_values\n",
    "                column_values_df = pd.concat([temp_df,column_values_df],axis='rows')\n",
    "            column_values_df = column_values_df.sort_values(column) \n",
    "            column_values_df = column_values_df.reset_index(drop=True)\n",
    "            # predict for mean values and PDP\n",
    "            PDP_prediction_for_column = pd.DataFrame(self.model_prediction(column_values_df),columns=[f'PDP_{column}_CAP']) # capacitance of that specific seed\n",
    "            PDP_prediction_for_column = PDP_prediction_for_column.map(lambda x: x * (self.target_scale_dict['MAX_CAP'] - self.target_scale_dict['MIN_CAP']) + self.target_scale_dict['MIN_CAP'])\n",
    "            all_PDP_prediction_results = pd.concat([PDP_prediction_for_column,all_PDP_prediction_results],axis='columns')\n",
    "            # scale mean value df\n",
    "            export_mean_value_df[column] = mean_value_df[column].map(lambda x: x * (self.feature_scale_dict[f'MAX_{column}'] - self.feature_scale_dict[f'MIN_{column}']) + self.feature_scale_dict[f'MIN_{column}'])\n",
    "        # accumulate results\n",
    "        all_PDP_prediction_results['SEED'] = self.random_seed\n",
    "        all_PDP_prediction_results['ORDER'] = self.complete_order_name\n",
    "        all_PDP_prediction_results['INDEX'] = all_PDP_prediction_results.index\n",
    "        all_PDP_prediction_results['PDPVALUES'] = column_values_list\n",
    "        self.all_PDP_prediction_results = all_PDP_prediction_results\n",
    "        # accumulate results\n",
    "        export_mean_value_df['SEED'] = self.random_seed\n",
    "        export_mean_value_df['TYPE'] = export_mean_value_df.index\n",
    "        export_mean_value_df['ORDER'] = self.complete_order_name\n",
    "        export_mean_value_df = export_mean_value_df.reset_index(drop=True)\n",
    "        self.PDP_mean_value_df = export_mean_value_df    \n",
    "        # export results\n",
    "        self.temp_feature_importance_results_df = self.all_PDP_prediction_results\n",
    "        self.temp_additional_plotting_value_df = self.PDP_mean_value_df\n",
    "\n",
    "    # fpi\n",
    "    def FPI_prediction_function(self):\n",
    "        self.FPI_prediction_df = pd.DataFrame()\n",
    "        # get all features\n",
    "        feature_column = self.dataframe_train_feature.columns\n",
    "        self.dataframe_accumulated_FPI_prediction = pd.DataFrame()\n",
    "        # apply FPI for all features\n",
    "        for feature in feature_column:\n",
    "            unshuffled_column = self.dataframe_test_feature[[feature]]\n",
    "            shuffled_column = self.dataframe_test_feature[[feature]].sample(frac=1,random_state=0).set_index(unshuffled_column.index) # shuffle \n",
    "            dataframe_FPI_prediction = self.dataframe_test_feature.copy()\n",
    "            dataframe_FPI_prediction[feature] = shuffled_column\n",
    "            FPI_prediction = self.model_prediction(self.dataframe_test_feature)\n",
    "            self.FPI_prediction = pd.DataFrame(self.model_prediction(dataframe_FPI_prediction),columns=['CAP'])\n",
    "            self.FPI_prediction = self.FPI_prediction.map(lambda x: (x * (self.target_scale_dict['MAX_CAP'] - self.target_scale_dict['MIN_CAP'])) + self.target_scale_dict['MIN_CAP'])\n",
    "            # results\n",
    "            self.y_mean = self.real.mean().values[0]\n",
    "            self.FPI_MAE = abs(self.real - self.FPI_prediction).mean().values[0] # mean absolute error\n",
    "            self.FPI_MSE = ((self.real - self.FPI_prediction)**2).mean().values[0] # mean squared error\n",
    "            self.FPI_RMSE = math.sqrt(((self.real - self.FPI_prediction)**2).mean().values[0]) # root mean squared error\n",
    "            self.FPI_R2 = 1 - (((self.real - self.FPI_prediction)**2).sum().values[0]/((self.real - self.y_mean)**2).sum().values[0]) # R2\n",
    "            self.FPI_MAPE = abs((self.real - self.FPI_prediction)/self.real).mean().values[0] # mean absolute percentage error\n",
    "            # accumulate results\n",
    "            self.dataframe_FPI_prediction = pd.DataFrame(\n",
    "                [[self.FPI_MAE,self.FPI_MSE,self.FPI_RMSE,self.FPI_R2,self.FPI_MAPE,(self.train_end_time-self.train_start_time)/60,self.random_seed,self.complete_order_name,feature]],\n",
    "                columns=['MAE','MSE','RMSE','R2','MAPE','TIME','SEED','ORDER','SHUFFLE']\n",
    "            )\n",
    "            self.dataframe_accumulated_FPI_prediction = pd.concat([self.dataframe_FPI_prediction,self.dataframe_accumulated_FPI_prediction],axis='rows')\n",
    "        # export results\n",
    "        self.temp_feature_importance_results_df = self.dataframe_accumulated_FPI_prediction\n",
    "        self.temp_additional_plotting_value_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from here onwards, classes have overridden functions based on their use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "novel method sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model, fit function, with added sorting\n",
    "def fit(self):\n",
    "    # keep track of time\n",
    "    self.train_start_time = time.time()\n",
    "    # get each model used\n",
    "    self.model_dict = {}\n",
    "    for model in self.combination_name_tuple:\n",
    "        self.model_dict[model] = {}\n",
    "    # accumulate prediction set\n",
    "    self.prediction_list = []\n",
    "    self.META_hold_out_prediction_list = []\n",
    "    # meta model training process\n",
    "    for count_for_train, (train_index_for_validation,test_index_for_validation) in enumerate(self.splits_for_training):\n",
    "        # splits\n",
    "        kfold_train_feature_df = self.dataframe_train_feature.iloc[train_index_for_validation]\n",
    "        kfold_train_target_df = self.dataframe_train_target.iloc[train_index_for_validation]\n",
    "        kfold_test_feature_df = self.dataframe_train_feature.iloc[test_index_for_validation]\n",
    "        kfold_test_target_df = self.dataframe_train_target.iloc[test_index_for_validation]\n",
    "        # train model\n",
    "        for count,model_name in enumerate(self.combination_name_tuple):\n",
    "            self.model_dict[model_name] = {}\n",
    "            # give the model random state of 0 if possible\n",
    "            # check for best parameter, try with random state 0\n",
    "            try:\n",
    "                estimator = self.combination_class_tuple[count]()\n",
    "                param_grid = self.parameters_dict[model_name].copy()\n",
    "                param_grid['random_state'] = [np.random.RandomState(0)]\n",
    "                k_fold_grid_search = sklearn.model_selection.GridSearchCV(estimator=estimator,param_grid=param_grid)\n",
    "                k_fold_grid_search.fit(kfold_train_feature_df,kfold_train_target_df[['CAP']]) \n",
    "            except:\n",
    "                estimator = self.combination_class_tuple[count]()\n",
    "                param_grid = self.parameters_dict[model_name].copy()\n",
    "                k_fold_grid_search = sklearn.model_selection.GridSearchCV(estimator=estimator,param_grid=param_grid)\n",
    "                k_fold_grid_search.fit(kfold_train_feature_df,kfold_train_target_df[['CAP']]) \n",
    "            model = self.combination_class_tuple[count](\n",
    "                **k_fold_grid_search.best_params_\n",
    "            )\n",
    "            model.fit(kfold_train_feature_df,kfold_train_target_df[['CAP']])\n",
    "            model_prediction_for_training_meta_model = model.predict(kfold_test_feature_df)\n",
    "            # accumulate results of model\n",
    "            self.model_dict[model_name][count_for_train] = {}\n",
    "            self.model_dict[model_name][count_for_train]['model'] = model\n",
    "            self.model_dict[model_name][count_for_train]['meta_feature'] = model_prediction_for_training_meta_model                \n",
    "        # stacking\n",
    "        META_hold_out_prediction_df = pd.DataFrame()\n",
    "        for count,key in enumerate(self.model_dict):\n",
    "            meta_feature_df = pd.DataFrame(self.model_dict[key][count_for_train]['meta_feature'],columns=[f'MODEL {count}'])\n",
    "            META_hold_out_prediction_df = pd.concat([META_hold_out_prediction_df,meta_feature_df],axis='columns')\n",
    "        self.META_hold_out_prediction_list.append(META_hold_out_prediction_df)\n",
    "    # meta model training set\n",
    "    self.META_train_df = pd.concat(self.META_hold_out_prediction_list,axis='rows').reset_index(drop=True)\n",
    "    # meta model\n",
    "    # new, sort the values\n",
    "    self.sorted_META_train_df = self.META_train_df.sort_values(by=self.META_train_df.first_valid_index(),axis='columns',ascending=self.ascending)\n",
    "    # check for best parameter, try with random state 0\n",
    "    try:\n",
    "        estimator = self.stack_meta_model_class()\n",
    "        param_grid = self.parameters_dict[self.stack_meta_model_name].copy()\n",
    "        param_grid['random_state'] = [np.random.RandomState(0)]\n",
    "        meta_model_grid_search = sklearn.model_selection.GridSearchCV(estimator=estimator,param_grid=param_grid)\n",
    "        meta_model_grid_search.fit(self.sorted_META_train_df.values,self.dataframe_train_target.values.ravel()) \n",
    "    except:\n",
    "        estimator = self.stack_meta_model_class()\n",
    "        param_grid = self.parameters_dict[self.stack_meta_model_name].copy()\n",
    "        meta_model_grid_search = sklearn.model_selection.GridSearchCV(estimator=estimator,param_grid=param_grid)\n",
    "        meta_model_grid_search.fit(self.sorted_META_train_df.values,self.dataframe_train_target.values.ravel()) \n",
    "    META_model = self.stack_meta_model_class(\n",
    "        **meta_model_grid_search.best_params_\n",
    "    )\n",
    "    META_model.fit(self.sorted_META_train_df.values,self.dataframe_train_target.values.ravel())\n",
    "    self.META_model = META_model\n",
    "    # refit base models\n",
    "    # the refit base model orders change\n",
    "    self.refit_base_model_dict = {}\n",
    "    for count,model_name in enumerate(self.combination_name_tuple):\n",
    "        # check for best parameter, try with random state 0\n",
    "        try:\n",
    "            estimator = self.combination_class_tuple[count]()\n",
    "            param_grid = self.parameters_dict[model_name].copy()\n",
    "            param_grid['random_state'] = [np.random.RandomState(0)]\n",
    "            refit_grid_search = sklearn.model_selection.GridSearchCV(estimator=estimator,param_grid=param_grid)\n",
    "            refit_grid_search.fit(self.META_train_df.values,self.dataframe_train_target.values.ravel()) \n",
    "        except:\n",
    "            estimator = self.combination_class_tuple[count]()\n",
    "            param_grid = self.parameters_dict[model_name].copy()\n",
    "            refit_grid_search = sklearn.model_selection.GridSearchCV(estimator=estimator,param_grid=param_grid)\n",
    "            refit_grid_search.fit(self.META_train_df.values,self.dataframe_train_target.values.ravel()) \n",
    "        self.refit_base_model_dict[f'refit{model_name}'] = {}\n",
    "        model = self.combination_class_tuple[count](\n",
    "            **refit_grid_search.best_params_\n",
    "        )\n",
    "        model.fit(self.dataframe_train_feature.values,self.dataframe_train_target.values.ravel())\n",
    "        self.refit_base_model_dict[f'refit{model_name}']['model'] = model\n",
    "        # add code name for sorting\n",
    "        self.refit_base_model_dict[f'refit{model_name}']['code_name'] = f'MODEL {count}'\n",
    "    self.train_end_time = time.time()\n",
    "\n",
    "# stacking model prediction function\n",
    "def model_prediction(self,features):\n",
    "    META_prediction_df = pd.DataFrame()\n",
    "    for count,refit_model in enumerate(self.refit_base_model_dict):\n",
    "        try:\n",
    "            refit_model_prediction = pd.DataFrame(self.refit_base_model_dict[refit_model]['model'].predict(features.values),columns=[self.refit_base_model_dict[refit_model]['code_name']])\n",
    "        except:\n",
    "            refit_model_prediction = pd.DataFrame(self.refit_base_model_dict[refit_model]['model'].predict(features),columns=[self.refit_base_model_dict[refit_model]['code_name']])\n",
    "        META_prediction_df = pd.concat([META_prediction_df,refit_model_prediction],axis='columns')\n",
    "    rearranged_column_list = self.sorted_META_train_df.columns\n",
    "    # new\n",
    "    self.sorted_META_prediction_df = pd.DataFrame()\n",
    "    for sorted_column in rearranged_column_list:\n",
    "        self.sorted_META_prediction_df[sorted_column] = META_prediction_df[sorted_column]  \n",
    "    # end new\n",
    "    try:\n",
    "        self.META_prediction = self.META_model.predict(self.sorted_META_prediction_df.values)\n",
    "    except:\n",
    "        self.META_prediction = self.META_model.predict(self.sorted_META_prediction_df)\n",
    "    return self.META_prediction     \n",
    "\n",
    "# class with overridden function\n",
    "class Sort_Stacking_Process(Main_Stacking_Process):\n",
    "  def __init__(self,base_model_names,base_model_classes,stack_meta_model_name,stack_meta_model_class,method,ascending):\n",
    "    self.ascending = ascending\n",
    "    super().__init__(base_model_names,base_model_classes,stack_meta_model_name,stack_meta_model_class,method)\n",
    "\n",
    "Sort_Stacking_Process.fit = fit\n",
    "Sort_Stacking_Process.model_prediction = model_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models used and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class '__main__.Sort_Stacking_Process'>, 'Descending')\n",
      "Start\n",
      "META: DT\n",
      "MODELS: ('DT', 'KNN', 'LIN')\n",
      "FPI\n",
      "PDP\n",
      "SHAP\n",
      "Start\n",
      "META: KNN\n",
      "MODELS: ('DT', 'KNN', 'LIN')\n",
      "FPI\n",
      "PDP\n",
      "SHAP\n",
      "Start\n",
      "META: LIN\n",
      "MODELS: ('DT', 'KNN', 'LIN')\n",
      "FPI\n",
      "PDP\n",
      "SHAP\n",
      "(<class '__main__.Sort_Stacking_Process'>, 'Ascending')\n",
      "Start\n",
      "META: DT\n",
      "MODELS: ('DT', 'KNN', 'LIN')\n",
      "FPI\n",
      "PDP\n",
      "SHAP\n",
      "Start\n",
      "META: KNN\n",
      "MODELS: ('DT', 'KNN', 'LIN')\n",
      "FPI\n",
      "PDP\n",
      "SHAP\n",
      "Start\n",
      "META: LIN\n",
      "MODELS: ('DT', 'KNN', 'LIN')\n",
      "FPI\n",
      "PDP\n",
      "SHAP\n",
      "(<class '__main__.Main_Stacking_Process'>, 'Handcode')\n",
      "Start\n",
      "META: DT\n",
      "MODELS: ('DT', 'KNN', 'LIN')\n",
      "FPI\n",
      "PDP\n",
      "SHAP\n",
      "Start\n",
      "META: KNN\n",
      "MODELS: ('DT', 'KNN', 'LIN')\n",
      "FPI\n",
      "PDP\n",
      "SHAP\n",
      "Start\n",
      "META: LIN\n",
      "MODELS: ('DT', 'KNN', 'LIN')\n",
      "FPI\n",
      "PDP\n",
      "SHAP\n"
     ]
    }
   ],
   "source": [
    "# base models and meta models\n",
    "stack_model_tuple = tuple(sorted(['KNN','DT','LIN'])) # number of choices (n)\n",
    "stack_model_class_dict = {'DT':DecisionTreeRegressor,'KNN':KNeighborsRegressor,'LIN':LinearRegression}\n",
    "stack_meta_model_tuple = tuple(sorted(['KNN','DT','LIN'])) # number of choices (n)\n",
    "stack_meta_model_class_dict = {'DT':DecisionTreeRegressor,'KNN':KNeighborsRegressor,'LIN':LinearRegression}\n",
    "\n",
    "true_depth = len(stack_model_tuple)\n",
    "\n",
    "# loop through all meta models and methods\n",
    "method_list = [\n",
    "    (Sort_Stacking_Process,'Descending'),    \n",
    "    (Sort_Stacking_Process,'Ascending'),\n",
    "    (Main_Stacking_Process,'Handcode'),\n",
    "]\n",
    "for method in method_list:\n",
    "    print(method)\n",
    "    for count in range(len(stack_meta_model_tuple)):\n",
    "        stack_meta_model_name = stack_meta_model_tuple[count]\n",
    "        stack_meta_model_class = stack_meta_model_class_dict[stack_meta_model_name]\n",
    "        if method[1] == 'Ascending':\n",
    "            process = method[0](base_model_names=stack_model_tuple,base_model_classes=stack_model_class_dict,stack_meta_model_name=stack_meta_model_name,stack_meta_model_class=stack_meta_model_class,method=method[1],ascending=True)\n",
    "        elif method[1] == 'Descending':\n",
    "            process = method[0](base_model_names=stack_model_tuple,base_model_classes=stack_model_class_dict,stack_meta_model_name=stack_meta_model_name,stack_meta_model_class=stack_meta_model_class,method=method[1],ascending=False)\n",
    "        else:\n",
    "            process = method[0](base_model_names=stack_model_tuple,base_model_classes=stack_model_class_dict,stack_meta_model_name=stack_meta_model_name,stack_meta_model_class=stack_meta_model_class,method=method[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
